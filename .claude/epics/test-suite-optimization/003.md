---
id: 003
title: Record current test execution metrics
epic: test-suite-optimization
status: ready
priority: critical
depends_on: []
parallel: ["001", "002"]
estimated_hours: 2
labels:
  - measurement
  - baseline
  - performance
---

# Task 003: Record current test execution metrics

## Objective

Capture comprehensive test execution metrics to establish performance baseline before optimizations.

## Background

We need baseline metrics for test execution time, flakiness, and resource usage to measure the impact of our optimizations. This will help us ensure we're improving performance without introducing regressions.

## Requirements

### Must Have
- [ ] Total test execution time
- [ ] Execution time per test file
- [ ] Test count by type (unit/integration/e2e)
- [ ] Flaky test identification
- [ ] Baseline report in .claude/metrics/

### Nice to Have
- [ ] Slowest 10 tests identified
- [ ] Parallel vs sequential execution comparison
- [ ] CI vs local execution time differences

## Implementation Details

### Step 1: Enable detailed test reporting
Configure Vitest for detailed metrics:
```javascript
// vitest.config.ts additions
{
  reporters: ['verbose', 'json'],
  outputFile: '.claude/metrics/test-run.json',
  logHeapUsage: true
}
```

### Step 2: Identify test categories
Count tests by naming pattern:
```bash
# Unit tests (.test.ts and .unit.test.ts)
find . -name "*.test.ts" -o -name "*.unit.test.ts" | wc -l

# Integration tests
find . -name "*.integration.test.ts" | wc -l

# E2E tests
find . -name "*.e2e.test.ts" | wc -l
```

### Step 3: Run timed test executions
Execute and time different test scenarios:
```bash
# Full suite
time pnpm test

# Unit tests only
time pnpm test:unit

# With coverage
time pnpm test:coverage

# Watch mode (manual timing for 10 minutes)
pnpm test:watch
```

### Step 4: Identify flaky tests
Run tests multiple times to identify flaky ones:
```bash
for i in {1..5}; do
  pnpm test --reporter=json > .claude/metrics/run-$i.json
done
# Compare results to find inconsistent tests
```

### Step 5: Generate baseline report
Create `.claude/metrics/baseline-execution-{timestamp}.json`:
```json
{
  "timestamp": "2025-09-19T10:00:00Z",
  "summary": {
    "total_tests": 234,
    "unit_tests": 150,
    "integration_tests": 60,
    "e2e_tests": 24,
    "total_time_seconds": 45,
    "avg_time_per_test_ms": 192
  },
  "flaky_tests": [
    "auth.integration.test.ts:45",
    "rate-limit.test.ts:23"
  ],
  "slowest_tests": [
    {"file": "heavy.test.ts", "time_ms": 5000},
    ...
  ],
  "mock_count": 359,
  "npm_scripts_count": 82
}
```

## Acceptance Criteria

- [ ] Test execution times captured for all scenarios
- [ ] Test counts by type documented
- [ ] Flaky tests identified and listed
- [ ] Slowest tests identified
- [ ] Baseline report generated in .claude/metrics/

## Testing

- Run timing commands multiple times for consistency
- Verify test counts match actual files
- Confirm flaky test detection is accurate
- Validate JSON report structure

## Risks

- Flaky tests may not show up in limited runs
- Timing may vary based on system load
- Different environments show different performance

## Dependencies

- Vitest test runner
- Unix timing tools
- JSON processing utilities

## Notes

Run this task in parallel with Tasks 001 and 002 for efficiency. This completes Phase 0 baseline capture.